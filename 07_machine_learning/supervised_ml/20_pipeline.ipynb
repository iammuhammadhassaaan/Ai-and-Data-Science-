{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning** _Day 87_\n",
    "##### Name: Muhammad Hassaan\n",
    "##### Date: July 28, 2024\n",
    "##### Email: muhammadhassaan7896@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline in Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, a pipeline is a sequence of data processing steps that are chained together to automate and streamline the machine learning workflow. A pipeline allows you to combine multiple data preprocessing and model training steps into a single object, making it easier to organize and manage your machine learning code.\n",
    "\n",
    "> Here are the key components of a pipeline:\n",
    "\n",
    "`Data Preprocessing Steps:` Pipelines typically start with data preprocessing steps, such as feature scaling, feature encoding, handling missing values, or dimensionality reduction. These steps ensure that the data is in the appropriate format and quality for model training.\n",
    "\n",
    "`Model Training:` After the data preprocessing steps, the pipeline includes the training of a machine learning model. This can be a classifier for classification tasks, a regressor for regression tasks, or any other type of model depending on the problem at hand.\n",
    "\n",
    "`Model Evaluation:` Once the model is trained, the pipeline often incorporates steps for evaluating its performance. This may involve metrics calculation, cross-validation, or any other evaluation technique to assess the model's effectiveness.\n",
    "\n",
    "`Predictions:` After the model has been evaluated, the pipeline allows you to make predictions on new, unseen data using the trained model. This step applies the same preprocessing steps used during training to the new data before generating predictions.\n",
    "\n",
    "> The main advantages of using pipelines in machine learning are:\n",
    "\n",
    "`Simplified Workflow:` Pipelines provide a clean and organized structure for defining and managing the sequence of steps involved in machine learning tasks. This makes it easier to understand, modify, and reproduce the workflow.\n",
    "\n",
    "`Avoiding Data Leakage: `Pipelines ensure that data preprocessing steps are applied consistently to both the training and testing data, preventing data leakage that could lead to biased or incorrect results.\n",
    "\n",
    "`Streamlined Model Deployment:` Pipelines allow you to encapsulate the entire workflow, including data preprocessing and model training, into a single object. This simplifies the deployment of your machine learning model, as the same pipeline can be applied to new data without the need to reapply each individual step.\n",
    "\n",
    "`Hyperparameter Tuning:` Pipelines can be combined with techniques like grid search or randomized search for hyperparameter tuning. This allows you to efficiently explore different combinations of hyperparameters for your models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "Overall, pipelines are a powerful tool for managing and automating the machine learning workflow, promoting code reusability, consistency, and efficiency. They help streamline the development and deployment of machine learning models, making it easier to iterate and experiment with different approaches.\n",
    "\n",
    "Here's an example of using a pipeline on the Titanic dataset to preprocess the data, train a model, and make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Titanic dataset from Seaborn\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = df['survived']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the column transformer for imputing missing values\n",
    "numeric_features = ['age', 'fare']\n",
    "categorical_features = ['pclass', 'sex', 'embarked']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with the preprocessor and RandomForestClassifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "\n",
    "In this example, we start by loading the Titanic dataset from Seaborn using sns.load_dataset('titanic'). We then select the relevant features and target variable (survived) to train our model. Next, we split the data into training and test sets using train_test_split from scikit-learn.\n",
    "\n",
    "The pipeline is created using the Pipeline class from scikit-learn. It consists of three steps:\n",
    "\n",
    "Data preprocessing step: The SimpleImputer is used to handle missing values by replacing them with the most frequent value in each column.\n",
    "\n",
    "Feature encoding step: The OneHotEncoder is used to encode categorical variables (`sex and embarked`) as binary features.\n",
    "\n",
    "Model training step: The RandomForestClassifier is used as the machine learning model for classification.\n",
    "\n",
    "We then fit the pipeline on the training data using pipeline.fit(`X_train, y_train`). Afterward, we make predictions on the test data using pipeline.predict(`X_test`).\n",
    "\n",
    "Finally, we calculate the accuracy score by comparing the predicted values (`y_pred`) with the actual values (`y_test`).\n",
    "\n",
    "Note that you may need to install Seaborn (pip install seaborn) if it's not already installed in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter Tunning in Pipeline**\n",
    "\n",
    "Hyperparameter tuning in a pipeline involves optimizing the hyperparameters of the different steps in the pipeline to find the best combination that maximizes the model's performance. Here's an example of hyperparameter tuning in a pipeline and selecting the best model on the Titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# load the data \n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# select features and target variable\n",
    "X = df[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = df['survived']\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# define the parameters to tune \n",
    "hyperparameters = {\n",
    "    'model__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'model__max_depth': [10, 20, 30],\n",
    "    'model__min_samples_split': [2, 5, 10, 15]\n",
    "}\n",
    "\n",
    "# Perfrom grid search CV\n",
    "grid = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# get the best model \n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
